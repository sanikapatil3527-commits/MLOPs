<!-- # ðŸ§­ MLOps Standards & Principles

> Objectif : rÃ©duire la friction entre dÃ©veloppement et production, garantir la qualitÃ©, la traÃ§abilitÃ© et la reproductibilitÃ© des modÃ¨les ML.

---

## ðŸ” Transition Friction

RÃ©duire les obstacles entre :
- recherche â†’ dÃ©veloppement
- dÃ©veloppement â†’ production

âœ… Bonnes pratiques :
- Utiliser des **templates de notebooks** standardisÃ©s :
  - connexion base de donnÃ©es
  - chargement des donnÃ©es
  - prÃ©-processing commun
  - structure dâ€™expÃ©rience
- Documentation claire et Ã  jour

ðŸ‘‰ RÃ©sultat attendu :
- onboarding rapide
- cohÃ©rence entre Ã©quipes
- reproductibilitÃ©

---

## ðŸ—‚ Version Control System (VCS)

> "Ce qui nâ€™est pas versionnÃ© nâ€™existe pas."

Ã€ versionner :
- âœ… code
- âœ… donnÃ©es (ou leur source)
- âœ… environnements
- âœ… artefacts (modÃ¨les, mÃ©triques, figures)

Outils recommandÃ©s :
- Git / GitHub / GitLab
- DVC
- MLflow Artifacts

---

## ðŸš€ Performance

Objectif : exÃ©cuter les pipelines de faÃ§on efficace et scalable

Approches :
- computing distribuÃ©
- containerisation

Outils :
- Docker
- Kubernetes
- Spark
- Ray

---

## ðŸ¤– Automation

> MLOps est **pipeline-centric**, pas model-centric

Objectifs :
- automatiser le passage du data au modÃ¨le en production
- CI/CD & CI/ML & CD/ML

Inclut :
- automatisation du training
- automatisation du dÃ©ploiement
- automatisation de l'Ã©valuation

Outils :
- GitHub Actions
- GitLab CI
- Jenkins
- MLflow Pipelines

---

## ðŸ“ˆ Monitoring

> Un modÃ¨le en production sans monitoring = bombe Ã  retardement

Ã€ monitorer :
- donnÃ©es entrantes
- distribution des features (drift)
- latence
- uptime
- mÃ©moire utilisÃ©e
- performance modÃ¨le

Outils recommandÃ©s :
- Prometheus
- Grafana
- MLflow Model Monitoring
- Evidently AI

---

## ðŸ”„ Continuous Training (CT)

Automatiser :
- le retraining rÃ©gulier
- ou basÃ© sur triggers :
  - drift dÃ©tectÃ©
  - nouveau dataset
  - seuil qualitÃ© dÃ©passÃ©

Pipeline typique :
Data â†’ Validation â†’ Training â†’ Evaluation â†’ Registry â†’ Deployment
â†‘ â†“
Monitoring â†----------â†

---

## âœ… RÃ©sumÃ© visuel

| Principe | Objectif | Outils |
|---|---|---|
| Transition Friction | Standardisation | Templates, Docs |
| Version Control | TraÃ§abilitÃ© | Git, DVC, MLflow |
| Performance | ScalabilitÃ© | Docker, K8s |
| Automation | Pipelines | CI/CD, MLflow |
| Monitoring | QualitÃ© prod | Prometheus, Grafana |
| Continuous Training | Adaptation | Auto retraining |

---

## ðŸ§  Message clÃ© pour le cours

> Le cÅ“ur du MLOps moderne (et de MLflow) nâ€™est pas seulement dâ€™entraÃ®ner un modÃ¨le, mais de maintenir **un pipeline vivant, monitorÃ© et automatisÃ©**.

--- -->

# ðŸ§­ MLOps Standards & Principles

> Objective: reduce friction between development and production, while ensuring model quality, traceability, and reproducibility.

---

## ðŸ” Transition Friction

Reduce barriers between:
- research â†’ development
- development â†’ production

âœ… Best practices:
- Use **standardized notebook templates**:
  - database connection
  - data loading
  - shared preprocessing
  - experiment structure
- Clear and up-to-date documentation

ðŸ‘‰ Expected outcomes:
- faster onboarding
- team consistency
- reproducibility

---

## ðŸ—‚ Version Control System (VCS)

> "If it is not versioned, it does not exist."

What should be versioned:
- âœ… code
- âœ… data (or data sources)
- âœ… environments
- âœ… artifacts (models, metrics, figures)

Recommended tools:
- Git / GitHub / GitLab
- DVC
- MLflow Artifacts

---

## ðŸš€ Performance

Objective: run pipelines efficiently and at scale

Approaches:
- distributed computing
- containerization

Tools:
- Docker
- Kubernetes
- Spark
- Ray

---

## ðŸ¤– Automation

> MLOps is **pipeline-centric**, not model-centric

Objectives:
- automate the path from data to production models
- CI/CD, CI/ML, and CD/ML

Includes:
- automated training
- automated deployment
- automated evaluation

Tools:
- GitHub Actions
- GitLab CI
- Jenkins
- MLflow Pipelines

---

## ðŸ“ˆ Monitoring

> A production model without monitoring is a ticking time bomb

What to monitor:
- incoming data
- feature distributions (data drift)
- latency
- uptime
- memory usage
- model performance

Recommended tools:
- Prometheus
- Grafana
- MLflow Model Monitoring
- Evidently AI

---

## ðŸ”„ Continuous Training (CT)

Automate:
- periodic retraining
- or trigger-based retraining:
  - detected drift
  - new datasets
  - quality thresholds exceeded

Typical pipeline:

Data â†’ Validation â†’ Training â†’ Evaluation â†’ Registry â†’ Deployment
â†‘ â†“
Monitoring â†-----------------------------â†
---

## âœ… Visual Summary

| Principle | Objective | Tools |
|---|---|---|
| Transition Friction | Standardization | Templates, Docs |
| Version Control | Traceability | Git, DVC, MLflow |
| Performance | Scalability | Docker, Kubernetes |
| Automation | Pipelines | CI/CD, MLflow |
| Monitoring | Production Quality | Prometheus, Grafana |
| Continuous Training | Adaptation | Auto-retraining |

---

## ðŸ§  Key Message for the Course

> The core of modern MLOps (and MLflow) is not just training a model,  
> but maintaining **a living, monitored, and automated pipeline**.

---
